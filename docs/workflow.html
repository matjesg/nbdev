---

title: Workflow


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/workflow.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/workflow.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Visit-Deepflash2-on-Google-Colab">Visit Deepflash2 on Google Colab<a class="anchor-link" href="#Visit-Deepflash2-on-Google-Colab"> </a></h2><p>For using deepflash2 in Google Colab follow this link: <a href="https://colab.research.google.com/github/matjesg/deepflash2/blob/master/nbs/deepflash2.ipynb">https://colab.research.google.com/github/matjesg/deepflash2/blob/master/nbs/deepflash2.ipynb</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup-Enviroment">Setup Enviroment<a class="anchor-link" href="#Setup-Enviroment"> </a></h2><p>At first you must allow google colab to run the notebook from GitHub by accepting the prompt.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Connect-to-your-google-drive">Connect to your google drive<a class="anchor-link" href="#Connect-to-your-google-drive"> </a></h2><p>It is recommended to connect your runtime to google drive.
You can allow it with ‘y’ for yes or deny it with ‘n’ for no.</p>
<p>Go to the Url that is presented to you in the “Set up environment cell”.
There you can choose your google drive account to connect to google colab.</p>
<p>After you have successfully connected the accounts, google will present you a one time authorization code. Copy this code and enter it in the according field in the “Set up environment cell” and continue by pressing the enter key.</p>
<p>Note: The authentication key will only work with this runtime. When you close google colab you have to request a new code as described.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Start-the-UI">Start the UI<a class="anchor-link" href="#Start-the-UI"> </a></h2><p>Go to the Url that is presented to you in the “Set up environment cell”.
There you can choose your google drive account to connect to google colab.</p>
<p>After you have successfully connected the accounts, google will present you a one time authorization code. Copy this code and enter it in the according field in the “Set up environment cell” and continue by pressing the enter key.</p>
<p>Note: The authentication key will only work with this runtime. When you close google colab you have to request a new code as described.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Ground-Truth-Estimation">Ground Truth Estimation<a class="anchor-link" href="#Ground-Truth-Estimation"> </a></h2><h3 id="Expert-Annotations">Expert Annotations<a class="anchor-link" href="#Expert-Annotations"> </a></h3><p>You can estimate the ground truth by selecting images that are segmented beforehand by different experts. We recommend at least 12 different images from 3 different experts. After you have selected the desired data, you can press Load Data.</p>
<p>Also you have the possibility to use sample data by clicking on “Load Sample Data”. There are 5 times 5 pictures available, coming from different experts</p>
<p><br></p>
<h3 id="Ground-Truth-Estimation">Ground Truth Estimation<a class="anchor-link" href="#Ground-Truth-Estimation"> </a></h3><p>When you have uploaded the images you can start the ground truth estimation by selecting one of the presented algorithms.
At the time of writing you can select between STAPLE and Majority Voting.
We recommend the STAPLE Algorithm when:</p>
<ul>
<li>if the experience of experts that have annotated the images vary or is unknown</li>
<li>if you need more precise results when compared with the Majority Voting Algorithm</li>
</ul>
<p>We recommend the Majority Voting Algorithm when:</p>
<ul>
<li>Use this algorithm if you can be sure that the expert annotations have no repeated errors.</li>
</ul>
<p>When the estimation is finished you can download the ground truth images for further use in training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-the-model">Training the model<a class="anchor-link" href="#Training-the-model"> </a></h2><p>Note: For comprehensive results you require a ground truth image that is according to the content found in the training images you want to use to train the neural network. E.G. if you want to train a neural network to find specific objects in the images, these objects should also be found in the expert images used for the ground truth estimation.</p>
<p>In this step you will create a model that you can use to automatically annotate new images.</p>
<p><br></p>
<h3 id="Data">Data<a class="anchor-link" href="#Data"> </a></h3><p>First, you have to provide training images. These should be unsegmented and contain the objects you want the neural network to find.
Second, you have to offer segmentation masks you have to create beforehand.
Third, you have to select a number of classes. The number of classes to choose depends on the characteristics of the segmentation. E.g., two for binary segmentation (foreground and background class).
Fourth, you can provide instance labels. This step is optional.</p>
<p><br></p>
<h3 id="Ensemble-Training">Ensemble Training<a class="anchor-link" href="#Ensemble-Training"> </a></h3><p>Note: We recommend that you reserve 70% of your images data for model training.</p>
<p>You can use the Ensemble training to optimize the results.
First choose a Number of models within an ensemble; If you're experimenting with parameters, try only one model first; Depending on the data, ensembles should at least comprise 3-5 models
How many times a single model is trained on a mini-batch of the training data.</p>
<p>Train all models (ensemble) or (re-)train specific model.</p>
<p><br></p>
<h3 id="Validation">Validation<a class="anchor-link" href="#Validation"> </a></h3><p>Note: We recommend that you reserve 30% of your image data for validation.</p>
<p>Here you can validate the performance of the model you have trained before with unsegmented images.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Prediction">Prediction<a class="anchor-link" href="#Prediction"> </a></h2><p>In this section you can use a model to segment the images and evaluate the results.</p>
<p><br></p>
<h3 id="Data-and-Ensemble">Data and Ensemble<a class="anchor-link" href="#Data-and-Ensemble"> </a></h3><p>First you have to upload the images you want the model to work with and predict results.
Second you have to select the model you want to apply.</p>
<p><br></p>
<h3 id="Prediction-and-Quality-Control">Prediction and Quality Control<a class="anchor-link" href="#Prediction-and-Quality-Control"> </a></h3><p>Here you can run the prediction and download the results.
You can enable test-time augmentation for prediction (more reliable and accurate, but slow).</p>

</div>
</div>
</div>
</div>
 

